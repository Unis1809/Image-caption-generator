{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import string\n",
    "from pickle import dump\n",
    "from pickle import load\n",
    "from keras.applications.xception import Xception #to get pre-trained model Xception\n",
    "from keras.applications.xception import preprocess_input\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.text import Tokenizer #for text tokenization\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Add\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense#Keras to build our CNN and LSTM\n",
    "from keras.layers import LSTM, Embedding, Dropout\n",
    "from tqdm import tqdm_notebook as tqdm #to check loop progress\n",
    "tqdm().pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# Load the document file into memory\n",
    "def load_fp(filename):\n",
    "    # Open file to read\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# Get all images with their captions\n",
    "def img_capt(filename):\n",
    "    file = load_fp(filename)\n",
    "    captions = file.split('\\n')\n",
    "    descriptions = {}\n",
    "    for caption in captions[:-1]:\n",
    "        img, caption = caption.split('\\t')\n",
    "        if img[:-2] not in descriptions:\n",
    "            descriptions[img[:-2]] = [caption]\n",
    "        else:\n",
    "            descriptions[img[:-2]].append(caption)\n",
    "    return descriptions\n",
    "\n",
    "# Data cleaning function will convert all upper case alphabets to lowercase, removing punctuations and words containing numbers\n",
    "def txt_clean(captions):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for img, caps in captions.items():\n",
    "        for i, img_caption in enumerate(caps):\n",
    "            img_caption.replace(\"-\", \" \")\n",
    "            desc = img_caption.split()\n",
    "            # Uppercase to lowercase\n",
    "            desc = [wrd.lower() for wrd in desc]\n",
    "            # Remove punctuation from each token\n",
    "            desc = [wrd.translate(table) for wrd in desc]\n",
    "            # Remove hanging 's and a\n",
    "            desc = [wrd for wrd in desc if (len(wrd) > 1)]\n",
    "            # Remove words containing numbers with them\n",
    "            desc = [wrd for wrd in desc if (wrd.isalpha())]\n",
    "            # Converting back to string\n",
    "            img_caption = ' '.join(desc)\n",
    "            captions[img][i] = img_caption\n",
    "    return captions\n",
    "\n",
    "def txt_vocab(descriptions):\n",
    "    # To build vocab of all unique words\n",
    "    vocab = set()\n",
    "    for key in descriptions.keys():\n",
    "        [vocab.update(d.split()) for d in descriptions[key]]\n",
    "    return vocab\n",
    "\n",
    "# To save all descriptions in one file\n",
    "def save_descriptions(descriptions, filename):\n",
    "    lines = list()\n",
    "    for key, desc_list in descriptions.items():\n",
    "        for desc in desc_list:\n",
    "            lines.append(key + '\\t' + desc )\n",
    "    data = \"\\n\".join(lines)\n",
    "    file = open(filename, \"w\")\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "\n",
    "# Set these paths according to the project folder in your system\n",
    "dataset_text = \"C:\\\\Users\\\\king\\\\Downloads\\\\Flickr8k_text\"\n",
    "dataset_images = \"C:\\\\Users\\\\king\\\\Downloads\\\\Flickr8k_Dataset\\\\Flicker8k_Dataset\"\n",
    "\n",
    "# To prepare our text data\n",
    "filename = dataset_text + \"\\\\\" + \"Flickr8k.token.txt\"\n",
    "\n",
    "# Loading the file that contains all data and mapping them into descriptions dictionary \n",
    "descriptions = img_capt(filename)\n",
    "print(\"Length of descriptions =\", len(descriptions))\n",
    "\n",
    "# Cleaning the descriptions\n",
    "clean_descriptions = txt_clean(descriptions)\n",
    "\n",
    "# To build vocabulary\n",
    "vocabulary = txt_vocab(clean_descriptions)\n",
    "print(\"Length of vocabulary =\", len(vocabulary))\n",
    "\n",
    "# Saving all descriptions in one file\n",
    "save_descriptions(clean_descriptions, \"descriptions.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(directory):\n",
    "    model = Xception(include_top=False, pooling='avg')\n",
    "    features = {}\n",
    "    for pic in tqdm(os.listdir(directory)):\n",
    "        file = directory + \"/\" + pic\n",
    "        image = Image.open(file)\n",
    "        image = image.resize((299, 299))\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "        # image = preprocess_input(image)\n",
    "        image = image / 127.5\n",
    "        image = image - 1.0\n",
    "        feature = model.predict(image)\n",
    "        features[pic] = feature\n",
    "    return features\n",
    "\n",
    "# 2048 feature vector\n",
    "features = extract_features(dataset_images)\n",
    "dump(features, open(\"features.p\", \"wb\"))\n",
    "\n",
    "# To directly load the features from the pickle file.\n",
    "features = load(open(\"features.p\", \"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load text document\n",
    "def load_doc(filename):\n",
    "    # Open file to read\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# Load the data\n",
    "def load_photos(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        photos = file.read().strip().split(\"\\n\")\n",
    "    return photos\n",
    "\n",
    "def load_clean_descriptions(filename, photos):\n",
    "    descriptions = {}\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            words = line.split()\n",
    "            if len(words) < 2:\n",
    "                continue\n",
    "            image, image_caption = words[0], ' '.join(words[1:])\n",
    "            if image in photos:\n",
    "                if image not in descriptions:\n",
    "                    descriptions[image] = []\n",
    "                descriptions[image].append(image_caption)\n",
    "    return descriptions\n",
    "\n",
    "def load_features(photos):\n",
    "    # Loading all features\n",
    "    all_features = load(open(\"features.p\", \"rb\"))\n",
    "    # Selecting only needed features\n",
    "    features = {k: all_features[k] for k in photos}\n",
    "    return features\n",
    "\n",
    "filename = \"C:\\\\Users\\\\king\\\\Downloads\\\\Flickr8k_text\\\\Flickr_8k.trainImages.txt\"\n",
    "# train = loading_data(filename)\n",
    "train_imgs = load_photos(filename)\n",
    "train_descriptions = load_clean_descriptions(\"descriptions.txt\", train_imgs)\n",
    "train_features = load_features(train_imgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dictionary to clear list of descriptions\n",
    "def dict_to_list(descriptions):\n",
    "    all_desc = []\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.append(d) for d in descriptions[key]]\n",
    "    return all_desc\n",
    "\n",
    "# Creating tokenizer class\n",
    "# This will vectorize text corpus\n",
    "# Each integer will represent a token in the dictionary\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "def create_tokenizer(descriptions):\n",
    "    desc_list = dict_to_list(descriptions)\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(desc_list)\n",
    "    return tokenizer\n",
    "\n",
    "# Give each word an index, and store that into tokenizer.p pickle file\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "dump(tokenizer, open('tokenizer.p', 'wb'))\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "vocab_size  # The size of our vocabulary is 7577 words.\n",
    "\n",
    "# Calculate maximum length of descriptions to decide the model structure parameters.\n",
    "def max_length(descriptions):\n",
    "    desc_list = dict_to_list(descriptions)\n",
    "    return max(len(d.split()) for d in desc_list)\n",
    "\n",
    "max_length = max_length(descriptions)\n",
    "max_length  # Max length of description is 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(descriptions, features, tokenizer, max_length, max_iterations=None):\n",
    "    iterations = 0\n",
    "    while max_iterations is None or iterations < max_iterations:\n",
    "        for key, description_list in descriptions.items():\n",
    "            # Retrieve photo features\n",
    "            feature = features[key][0]\n",
    "            inp_image, inp_seq, op_word = create_sequences(tokenizer, max_length, description_list, feature)\n",
    "            yield [[inp_image, inp_seq], op_word]\n",
    "        \n",
    "def create_sequences(tokenizer, max_length, desc_list, feature):\n",
    "    x_1, x_2, y = list(), list(), list()\n",
    "    # Move through each description for the image\n",
    "    for desc in desc_list:\n",
    "        # Encode the sequence\n",
    "        seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "        # Divide one sequence into various X,y pairs\n",
    "        for i in range(1, len(seq)):\n",
    "            # Divide into input and output pair\n",
    "            in_seq, out_seq = seq[:i], seq[i]\n",
    "            # Pad input sequence\n",
    "            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "            # Encode output sequence\n",
    "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "            # Store\n",
    "            x_1.append(feature)\n",
    "            x_2.append(in_seq)\n",
    "            y.append(out_seq)\n",
    "    return np.array(x_1), np.array(x_2), np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, Dropout, add\n",
    "from keras.metrics import categorical_accuracy\n",
    "\n",
    "\n",
    "def define_model(vocab_size, max_length):\n",
    "    # Features from the CNN model compressed from 2048 to 256 nodes\n",
    "    inputs1 = Input(shape=(2048,))\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(256, activation='relu')(fe1)\n",
    "\n",
    "    # LSTM sequence model\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    se3 = LSTM(512, return_sequences=True)(se2)  # Increased units and added return_sequences=True\n",
    "    se4 = LSTM(512)(se3)  # Additional LSTM layer\n",
    "    se5 = Dense(256, activation='relu')(se4)\n",
    "    # Merging both models\n",
    "    decoder1 = add([fe2, se5])  # Connected to the output of the additional LSTM layer\n",
    "    decoder2 = Dense(512, activation='relu')(decoder1)  # Increased units\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\n",
    "    # Merge it [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[accuracy])\n",
    "\n",
    "    # Summarize model\n",
    "    print(model.summary())\n",
    "    plot_model(model, to_file='model.png', show_shapes=True)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the validation data\n",
    "filename_val = dataset_text + \"/\" + \"Flickr_8k.devImages.txt\"\n",
    "val_imgs = load_photos(filename_val)\n",
    "val_descriptions = load_clean_descriptions(\"descriptions.txt\", val_imgs)\n",
    "val_features = load_features(val_imgs)\n",
    "\n",
    "# Validation data generator\n",
    "val_steps = len(val_descriptions)\n",
    "val_generator = data_generator(val_descriptions, val_features, tokenizer, max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau, Callback\n",
    "\n",
    "# Custom callback to print loss and accuracy after each epoch\n",
    "class LossAccuracyLogger(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} - Loss: {logs['loss']:.4f} - Accuracy: {logs['accuracy']:.4f}\")\n",
    "\n",
    "# Print dataset information\n",
    "print('Dataset:', len(train_imgs))\n",
    "print('Descriptions (train):', len(train_descriptions))\n",
    "print('Photos (train):', len(train_features))\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Description Length:', max_length)\n",
    "\n",
    "# Create the model\n",
    "model = define_model(vocab_size, max_length)\n",
    "\n",
    "# Number of epochs\n",
    "epochs = 10\n",
    "\n",
    "# Steps per epoch\n",
    "steps = len(train_descriptions)\n",
    "\n",
    "\n",
    "# Define callbacks\n",
    "checkpoint = ModelCheckpoint(\"models/model6_{epoch}.h5\", monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "csv_logger = CSVLogger('training.log')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001)\n",
    "loss_accuracy_logger = LossAccuracyLogger()  # Custom callback\n",
    "train_generator = data_generator(train_descriptions, train_features, tokenizer, max_length, max_iterations=max_iterations)\n",
    "# Train the model for the specified number of epochs\n",
    "model.fit_generator(train_generator, epochs=epochs, steps_per_epoch=steps,\n",
    "                    callbacks=[checkpoint, csv_logger, reduce_lr, loss_accuracy_logger], verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback, CSVLogger, ReduceLROnPlateau\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from tqdm import tqdm\n",
    "\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            if index == tokenizer.word_index['end']:  # Check if it's the index of the end token\n",
    "                return 'end'\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "class LossAccuracyBLEULogger(Callback):\n",
    "    def __init__(self, val_descriptions, val_features, tokenizer, max_length, epochs):\n",
    "        super().__init__()\n",
    "        self.val_descriptions = val_descriptions\n",
    "        self.val_features = val_features\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.epochs = epochs\n",
    "        self.best_bleu_score = None\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Print loss and accuracy\n",
    "        print(f\"Epoch {epoch + 1}/{self.epochs} - Loss: {logs['loss']:.4f} - Accuracy: {logs['accuracy']:.4f}\")\n",
    "        \n",
    "        # Evaluate BLEU score on validation set\n",
    "        bleu_score, predicted_captions = self.evaluate_bleu()\n",
    "        print(f\"BLEU Score: {bleu_score:.4f}\")\n",
    "\n",
    "        # Print the first 50 predicted captions\n",
    "        print(\"First 50 Predicted Captions:\")\n",
    "        for i, caption in enumerate(predicted_captions[:50]):\n",
    "            print(f\"{i+1}. {caption}\")\n",
    "\n",
    "        # Stop training if BLEU score does not improve\n",
    "        if self.best_bleu_score is None or bleu_score > self.best_bleu_score:\n",
    "            self.best_bleu_score = bleu_score\n",
    "        else:\n",
    "            self.model.stop_training = True\n",
    "\n",
    "    def evaluate_bleu(self):\n",
    "        actual, predicted = [], []\n",
    "        predicted_captions = []  # to store the first 50 predicted captions\n",
    "        desc_items = list(self.val_descriptions.items())\n",
    "        for key, desc_list in tqdm(desc_items, desc=\"Calculating BLEU Score\"):\n",
    "            image_feature = self.val_features[key][0].reshape((1, 2048))\n",
    "            generated_caption = self.generate_caption(image_feature)\n",
    "            actual.append([desc.split() for desc in desc_list])\n",
    "            predicted.append(generated_caption.split())\n",
    "            predicted_captions.append(generated_caption)\n",
    "        bleu_score = corpus_bleu(actual, predicted)\n",
    "        return bleu_score, predicted_captions\n",
    "\n",
    "    def generate_caption(self, photo_feature):\n",
    "        in_text = 'start'\n",
    "        for _ in range(self.max_length):\n",
    "            sequence = self.tokenizer.texts_to_sequences([in_text])[0]\n",
    "            sequence = pad_sequences([sequence], maxlen=self.max_length)\n",
    "            yhat = self.model.predict([photo_feature, sequence], verbose=0)\n",
    "            yhat = np.argmax(yhat)\n",
    "            word = word_for_id(yhat, self.tokenizer)\n",
    "            if word is None:\n",
    "             break\n",
    "            in_text+=' ' + word\n",
    "            if word =='end':\n",
    "             break\n",
    "            \n",
    "        return in_text\n",
    "\n",
    "# Print dataset information\n",
    "print('Dataset:', len(val_imgs))\n",
    "print('Descriptions (val):', len(val_descriptions))\n",
    "print('Photos (val):', len(val_features))\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Description Length:', max_length)\n",
    "\n",
    "# Create the model\n",
    "model = define_model(vocab_size, max_length)\n",
    "\n",
    "# Number of epochs\n",
    "epochs = 10\n",
    "\n",
    "# Steps per epoch\n",
    "steps = len(val_descriptions)\n",
    "\n",
    "# Define callbacks\n",
    "csv_logger = CSVLogger('val.log')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001)\n",
    "loss_accuracy_bleu_logger = LossAccuracyBLEULogger(val_descriptions, val_features, tokenizer, max_length, epochs)\n",
    "val_generator = data_generator(val_descriptions, val_features, tokenizer, max_length)\n",
    "\n",
    "# Train the model for the specified number of epochs\n",
    "model.fit_generator(val_generator, epochs=epochs, steps_per_epoch=steps,\n",
    "                    validation_data=val_generator, validation_steps=val_steps,\n",
    "                    callbacks=[csv_logger, reduce_lr, loss_accuracy_bleu_logger], verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test data\n",
    "filename_test = dataset_text + \"/\" + \"Flickr_8k.testImages.txt\"\n",
    "test_imgs = load_photos(filename_test)\n",
    "test_descriptions = load_clean_descriptions(\"descriptions.txt\", test_imgs)\n",
    "test_features = load_features(test_imgs)\n",
    "\n",
    "# Test data generator\n",
    "test_steps = len(test_descriptions)\n",
    "test_generator = data_generator(test_descriptions, test_features, tokenizer, max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to display image with caption\n",
    "def display_image_with_caption(image_path, caption):\n",
    "    img = mpimg.imread(image_path)\n",
    "    plt.imshow(img)\n",
    "    plt.title(caption)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Function to load tokenizer\n",
    "def load_tokenizer(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        tokenizer = pickle.load(f)\n",
    "    return tokenizer\n",
    "\n",
    "# Function to generate a description for an image\n",
    "def generate_description(model, tokenizer, photo_feature, max_length):\n",
    "    generated_description = 'start'\n",
    "    for _ in range(max_length):\n",
    "        sequence = tokenizer.texts_to_sequences([generated_description])[0]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        yhat = model.predict([photo_feature, sequence], verbose=0)\n",
    "        yhat = np.argmax(yhat)\n",
    "        word = word_for_id(yhat, tokenizer)\n",
    "        \n",
    "        # Stopping condition 1: If the predicted word is None, break the loop\n",
    "        if word is None:\n",
    "            break\n",
    "        \n",
    "        # Stopping condition 2: If the predicted word is 'endseq', break the loop\n",
    "        if word == 'end':\n",
    "            break\n",
    "        \n",
    "        # Check grammar before adding the word to the description\n",
    "        generated_description += ' ' + word\n",
    "    \n",
    "    return generated_description\n",
    "\n",
    "# Load the test data\n",
    "filename_test = dataset_text + \"/\" + \"Flickr_8k.testImages.txt\"\n",
    "test_imgs = load_photos(filename_test)\n",
    "test_descriptions = load_clean_descriptions(\"descriptions.txt\", test_imgs)\n",
    "test_features = load_features(test_imgs)\n",
    "\n",
    "# Generate captions for test dataset\n",
    "test_actual_captions = []\n",
    "test_predicted_captions = []\n",
    "for key in tqdm(test_imgs, desc=\"Generating Captions\"):\n",
    "    photo_feature = test_features[key].reshape((1, 2048))\n",
    "    actual_caption = ' '.join(test_descriptions[key])\n",
    "    predicted_caption = generate_description(model, tokenizer, photo_feature, max_length)\n",
    "    test_actual_captions.append(actual_caption)\n",
    "    test_predicted_captions.append(predicted_caption)\n",
    "\n",
    "# Calculate BLEU score\n",
    "actual = [[desc.split()] for desc in test_actual_captions]\n",
    "predicted = [desc.split() for desc in test_predicted_captions]\n",
    "best_bleu_score = 0\n",
    "for _ in range(epochs):  # Run for a fixed number of epochs\n",
    "    bleu_score = corpus_bleu(actual, predicted)\n",
    "    if bleu_score > best_bleu_score:\n",
    "        best_bleu_score = bleu_score\n",
    "        best_predicted_captions = test_predicted_captions.copy()  # Keep track of best predictions\n",
    "    # Train model for another epoch here\n",
    "\n",
    "# Display images with the best predicted captions\n",
    "print(\"Examples with the best predicted captions:\")\n",
    "for i, actual_caption in enumerate(test_actual_captions):\n",
    "    image_path = 'C:\\\\Users\\\\king\\\\Downloads\\\\Flickr8k_Dataset\\\\Flicker8k_Dataset\\\\' + test_imgs[i]   # Adjust the path to your images directory\n",
    "    predicted_caption = best_predicted_captions[i]\n",
    "    display_image_with_caption(image_path, f\"Actual: {actual_caption}\\nPredicted: {predicted_caption}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.xception import preprocess_input\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "from pickle import load\n",
    "\n",
    "# Assign the image path directly\n",
    "img_path = \"C:\\\\Users\\\\king\\\\Downloads\\\\xx.jpg\"\n",
    "\n",
    "def extract_features(filename, model):\n",
    "    try:\n",
    "        image = load_img(filename, target_size=(299, 299))\n",
    "    except:\n",
    "        print(\"ERROR: Can't open image! Ensure that image path and extension is correct\")\n",
    "    \n",
    "    image = img_to_array(image)\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    image = preprocess_input(image)\n",
    "    \n",
    "    feature = model.predict(image)\n",
    "    return feature\n",
    "\n",
    "import language_tool_python\n",
    "\n",
    "# Create a Grammar Checker instance\n",
    "grammar_checker = language_tool_python.LanguageTool('en-US')\n",
    "\n",
    "def generate_desc(model, tokenizer, photo_feature, max_length):\n",
    "    generated_description = 'start'\n",
    "    used_words = set()  # Initialize a set to store used words\n",
    "    for _ in range(max_length):\n",
    "        sequence = tokenizer.texts_to_sequences([generated_description])[0]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        yhat = model.predict([photo_feature, sequence], verbose=0)\n",
    "        yhat = np.argmax(yhat)\n",
    "        word = word_for_id(yhat, tokenizer)\n",
    "        \n",
    "        # Stopping condition 1: If the predicted word is None, break the loop\n",
    "        if word is None:\n",
    "            break\n",
    "        \n",
    "        # Stopping condition 2: If the predicted word is 'endseq', break the loop\n",
    "        if word == 'end':\n",
    "            break\n",
    "        \n",
    "        # Check if the word has already been used in the description\n",
    "        if word in used_words:\n",
    "            continue  # Skip the word and predict the next one\n",
    "        \n",
    "        # Add the word to the set of used words\n",
    "        used_words.add(word)\n",
    "        \n",
    "        # Check grammar before adding the word to the description\n",
    "        corrected_word = grammar_checker.correct(word)\n",
    "        \n",
    "        generated_description += ' ' + corrected_word\n",
    "    \n",
    "    return generated_description\n",
    "\n",
    "max_length = 32\n",
    "\n",
    "\n",
    "xception_model = Xception(include_top=False, pooling=\"avg\")\n",
    "photo = extract_features(img_path, xception_model)\n",
    "img = Image.open(img_path)\n",
    "description = generate_desc(model, tokenizer, photo, max_length)\n",
    "print(\"\\nGenerated Description:\\n\", description)\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
